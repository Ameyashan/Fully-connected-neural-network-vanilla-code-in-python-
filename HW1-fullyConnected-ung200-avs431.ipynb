{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  [[0.23137255 0.60392157 1.         ... 0.1372549  0.74117647 0.89803922]\n",
      " [0.16862745 0.49411765 0.99215686 ... 0.15686275 0.72941176 0.9254902 ]\n",
      " [0.19607843 0.41176471 0.99215686 ... 0.16470588 0.7254902  0.91764706]\n",
      " ...\n",
      " [0.54901961 0.54509804 0.3254902  ... 0.30196078 0.6627451  0.67843137]\n",
      " [0.32941176 0.55686275 0.3254902  ... 0.25882353 0.67058824 0.63529412]\n",
      " [0.28235294 0.56470588 0.32941176 ... 0.19607843 0.67058824 0.63137255]]\n",
      "y_train shape:  (50000,)\n",
      "{b'airplane': 0, b'automobile': 1, b'bird': 2, b'cat': 3, b'deer': 4, b'dog': 5, b'frog': 6, b'horse': 7, b'ship': 8, b'truck': 9}\n",
      "Metrics for Iteration 0\n",
      "     Cost: 1.9213975227274473\n",
      "     Training accuracy: 36.713 percent\n",
      "     Validation accuracy: 36.580 percent\n",
      "Metrics for Iteration 100\n",
      "     Cost: 0.4259693272242243\n",
      "     Training accuracy: 71.727 percent\n",
      "     Validation accuracy: 84.280 percent\n",
      "Metrics for Iteration 200\n",
      "     Cost: 0.1174498812679115\n",
      "     Training accuracy: 78.240 percent\n",
      "     Validation accuracy: 91.680 percent\n",
      "Metrics for Iteration 300\n",
      "     Cost: 0.11863453237614012\n",
      "     Training accuracy: 82.807 percent\n",
      "     Validation accuracy: 94.680 percent\n",
      "Metrics for Iteration 400\n",
      "     Cost: 0.027576873825546827\n",
      "     Training accuracy: 87.569 percent\n",
      "     Validation accuracy: 96.460 percent\n",
      "Metrics for Iteration 500\n",
      "     Cost: 0.08330369204333093\n",
      "     Training accuracy: 87.351 percent\n",
      "     Validation accuracy: 96.300 percent\n",
      "Metrics for Iteration 600\n",
      "     Cost: 0.06092175652809502\n",
      "     Training accuracy: 93.011 percent\n",
      "     Validation accuracy: 99.040 percent\n",
      "Metrics for Iteration 700\n",
      "     Cost: 0.07067867473129953\n",
      "     Training accuracy: 90.473 percent\n",
      "     Validation accuracy: 97.140 percent\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Abstraction of neural network.\n",
    "    Stores parameters, activations, cached values. \n",
    "    Provides necessary functions for training and prediction. \n",
    "    \"\"\"\n",
    "    def __init__(self, layer_dimensions, drop_prob=0.0, reg_lambda=0.0, momentum_beta=0, rms_beta=0, adam_beta1=0, adam_beta2=0, data_augmentation=False):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases for each layer\n",
    "        :param layer_dimensions: (list) number of nodes in each layer\n",
    "        :param drop_prob: drop probability for dropout layers. Only required in part 2 of the assignment\n",
    "        :param reg_lambda: regularization parameter. Only required in part 2 of the assignment\n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        self.parameters = {}\n",
    "        self.num_layers = len(layer_dimensions) - 1\n",
    "        \n",
    "        # init parameters\n",
    "        for layer in range(1, self.num_layers + 1):\n",
    "            w = np.divide(np.random.normal(0, 1, (layer_dimensions[layer], layer_dimensions[layer - 1])), np.sqrt(layer_dimensions[layer - 1]))\n",
    "            b = np.zeros(layer_dimensions[layer])\n",
    "            \n",
    "            self.parameters[layer] = [w, b]\n",
    "\n",
    "    def affineForward(self, A, W, b):\n",
    "        \"\"\"\n",
    "        Forward pass for the affine layer.\n",
    "        :param A: input matrix, shape (L, S), where L is the number of hidden units in the previous layer and S is\n",
    "        the number of samples\n",
    "        :returns: the affine product WA + b, along with the cache required for the backward pass\n",
    "        \"\"\"\n",
    "        Z = np.dot(W, A)\n",
    "        for i in range(len(Z)):\n",
    "            Z[i] = Z[i] + b[i]\n",
    "        return Z, [W, A, b, Z]\n",
    "        \n",
    "    def activationForward(self, A, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Common interface to access all activation functions.\n",
    "        :param A: input to the activation function\n",
    "        :param prob: activation funciton to apply to A. Just \"relu\" for this assignment.\n",
    "        :returns: activation(A)\n",
    "        \"\"\" \n",
    "        if(activation == \"relu\"):\n",
    "            ret = self.relu(A)\n",
    "        return ret\n",
    "\n",
    "    def relu(self, X):\n",
    "        A = np.maximum(0, X)\n",
    "        return A\n",
    "\n",
    "    def forwardPropagation(self, X):\n",
    "        \"\"\"\n",
    "        Runs an input X through the neural network to compute activations\n",
    "        for all layers. Returns the output computed at the last layer along\n",
    "        with the cache required for backpropagation.\n",
    "        :returns: (tuple) AL, cache\n",
    "            WHERE \n",
    "            AL is activation of last layer\n",
    "            cache is cached values for each layer that\n",
    "                     are needed in further steps\n",
    "        \"\"\"\n",
    "        cache = []\n",
    "        cache.append([]) # Empty cache for layer 1\n",
    "        Z, cacheLayer = self.affineForward(X, self.parameters[1][0], self.parameters[1][1])\n",
    "        A = self.activationForward(Z)\n",
    "\n",
    "        cache.append(cacheLayer)\n",
    "        for layer in range(2, self.num_layers):\n",
    "            Z, cacheLayer = self.affineForward(A, self.parameters[layer][0], self.parameters[layer][1])\n",
    "            A = self.activationForward(Z)\n",
    "            cache.append(cacheLayer)\n",
    "        Z, cacheLayer = self.affineForward(A, self.parameters[self.num_layers][0], self.parameters[self.num_layers][1])\n",
    "        AL = self.softmax(Z)\n",
    "        cache.append(cacheLayer)\n",
    "        \n",
    "        return AL, cache\n",
    "    \n",
    "    def costFunction(self, AL, y):\n",
    "        \"\"\"\n",
    "        :param AL: Activation of last layer, shape (num_classes, S)\n",
    "        :param y: labels, shape (S)\n",
    "        :param alpha: regularization parameter\n",
    "        :returns cost, dAL: A scalar denoting cost and the gradient of cost\n",
    "        \"\"\"\n",
    "        # compute loss\n",
    "        m = y.shape[0]\n",
    "        correct_label_prob = AL[y, range(m)]\n",
    "        cost = -np.sum(np.log(correct_label_prob)) / m\n",
    "                \n",
    "        \n",
    "        dAL = AL\n",
    "        dAL[y, range(AL.shape[1])] = dAL[y, range(AL.shape[1])] - 1\n",
    "        return cost, dAL\n",
    "    \n",
    "    def softmax(self, X):\n",
    "        return np.exp(X) / np.sum(np.exp(X), axis = 0)\n",
    "\n",
    "    def affineBackward(self, dA_prev, cache):\n",
    "        \"\"\"\n",
    "        Backward pass for the affine layer.\n",
    "        :param dA_prev: gradient from the next layer.\n",
    "        :param cache: cache returned in affineForward\n",
    "        :returns dA: gradient on the input to this layer\n",
    "                 dW: gradient on the weights\n",
    "                 db: gradient on the bias\n",
    "        \"\"\"\n",
    "        W = cache[0]\n",
    "        A = cache[1]\n",
    "        b = cache[2]\n",
    "        Z = cache[3]\n",
    "\n",
    "        dZ_prev = np.multiply(dA_prev, self.relu_derivative(Z))\n",
    "        dA = np.dot(W.transpose(), dZ_prev)\n",
    "        dW = np.dot(dZ_prev, A.transpose())\n",
    "        db = np.mean(dZ_prev, axis = 1) # Aggregate samples\n",
    "        \n",
    "        return dA, dW, db\n",
    "    \n",
    "    def affineBackwardLastLayer(self, dA_prev, Y, cache):\n",
    "        W = cache[0]\n",
    "        A = cache[1]\n",
    "        b = cache[2]\n",
    "        Z = cache[3]\n",
    "        dZ_prev = dA_prev\n",
    "        dA = np.dot(W.transpose(), dZ_prev)\n",
    "        dW = np.dot(dZ_prev, A.transpose())\n",
    "        db = np.mean(dZ_prev, axis = 1) # Aggregate samples\n",
    "        \n",
    "        return dA, dW, db\n",
    "\n",
    "        \n",
    "    def relu_derivative(self, cached_x):\n",
    "        relu_d = 1 * (cached_x > 0)\n",
    "        return relu_d\n",
    "\n",
    "\n",
    "    def backPropagation(self, dAL, Y, cache):\n",
    "        \"\"\"\n",
    "        Run backpropagation to compute gradients on all paramters in the model\n",
    "        :param dAL: gradient on the last layer of the network. Returned by the cost function.\n",
    "        :param Y: labels\n",
    "        :param cache: cached values during forwardprop\n",
    "        :returns gradients: dW and db for each weight/bias\n",
    "        \"\"\"\n",
    "        gradients = {}\n",
    "        dA, dW, db = self.affineBackwardLastLayer(dAL, Y, cache[self.num_layers])\n",
    "        gradients[self.num_layers] = [dW, db]\n",
    "        \n",
    "        for i in range(self.num_layers - 1):\n",
    "            layer = self.num_layers - 1 - i\n",
    "            dA, dW, db = self.affineBackward(dA, cache[layer])\n",
    "            gradients[layer] = [dW, db]\n",
    "        return gradients\n",
    "\n",
    "    def updateParameters(self, gradients, alpha):\n",
    "        \"\"\"\n",
    "        :param gradients: gradients for each weight/bias\n",
    "        :param alpha: step size for gradient descent \n",
    "        \"\"\"\n",
    "        for layer in range(1, self.num_layers + 1):\n",
    "            self.parameters[layer][0] = self.parameters[layer][0] - alpha * gradients[layer][0]\n",
    "            self.parameters[layer][1] = self.parameters[layer][1] - alpha * gradients[layer][1]\n",
    "    \n",
    "    def calculateAccuracy(self, y_actual, y_prediction):\n",
    "        correct = 0\n",
    "        for i in range(len(y_actual)):\n",
    "            if y_prediction[i] == y_actual[i]:\n",
    "                correct = correct + 1\n",
    "        accuracy = correct / len(y_actual) * 100\n",
    "        return accuracy\n",
    "    \n",
    "\n",
    "\n",
    "    def train(self, X, y, iters=1000, alpha=0.0001, batch_size=100, print_every=100):\n",
    "        \"\"\"\n",
    "        :param X: input samples, each column is a sample\n",
    "        :param y: labels for input samples, y.shape[0] must equal X.shape[1]\n",
    "        :param iters: number of training iterations\n",
    "        :param alpha: step size for gradient descent\n",
    "        :param batch_size: number of samples in a minibatch\n",
    "        :param print_every: no. of iterations to print debug info after\n",
    "        \"\"\"\n",
    "        no_of_examples = X.shape[1]\n",
    "        no_of_batches = int(no_of_examples / batch_size)\n",
    "        \n",
    "        # Split into training and validation sets\n",
    "        splitIndex = int(.9 * len(X[0])) # 90% train, 10% validation\n",
    "        X_train = X[:, :splitIndex]\n",
    "        y_train = y[:splitIndex]\n",
    "        X_validation = X[:, splitIndex:]\n",
    "        y_validation = y[splitIndex:]\n",
    "        \n",
    "\n",
    "        for iteration in range(0, iters):\n",
    "            self.parameters['batch_index'] = 0\n",
    "            for batches in range(no_of_batches):\n",
    "                # get minibatch\n",
    "                X_batch, Y_batch = self.get_batch(X, y, batch_size)\n",
    "\n",
    "                # forward prop\n",
    "                AL, cache = self.forwardPropagation(X_batch)\n",
    "\n",
    "                # compute loss\n",
    "                cost, dAL = self.costFunction(AL, Y_batch)\n",
    "\n",
    "                # compute gradients\n",
    "                gradients = self.backPropagation(dAL, Y_batch, cache)\n",
    "\n",
    "                # update weights and biases based on gradient\n",
    "                self.updateParameters(gradients, alpha)\n",
    "\n",
    "            if iteration % print_every == 0:\n",
    "                # print cost, train and validation set accuracies\n",
    "                print(\"Metrics for Iteration \" + str(iteration))\n",
    "                \n",
    "                # cost\n",
    "                print(\"     Cost: \" + str(cost))\n",
    "                \n",
    "                # train accuracy\n",
    "                y_train_prediction = self.predict(X_train)\n",
    "                train_accuracy = self.calculateAccuracy(y_train, y_train_prediction)\n",
    "                print(\"     Training accuracy: \" + \"{0:.3f}\".format(train_accuracy) + \" percent\")\n",
    "                \n",
    "                # validation accuracy\n",
    "                y_validation_prediction = self.predict(X_validation)\n",
    "                validation_accuracy = self.calculateAccuracy(y_validation, y_validation_prediction)\n",
    "                print(\"     Validation accuracy: \" + \"{0:.3f}\".format(validation_accuracy) + \" percent\")\n",
    "                \n",
    "                \n",
    "\n",
    "               \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for each sample\n",
    "        \"\"\"\n",
    "        AL, cache = self.forwardPropagation(X)\n",
    "        y_pred = np.argmax(AL, axis = 0)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def get_batch(self, X, y, batch_size):\n",
    "        \n",
    "        current_index=self.parameters[\"batch_index\"]\n",
    "        self.parameters[\"batch_index\"]=self.parameters[\"batch_index\"]+batch_size\n",
    "        X_batch,y_batch = X[:,current_index:current_index+batch_size], y[current_index:current_index+batch_size]\n",
    "\n",
    "        return X_batch, y_batch\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \n",
    "    y_one_hot = np.zeros((num_classes, y.shape[0]))\n",
    "    y_one_hot[y, range(y.shape[0])] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)\n",
    "    \n",
    "def get_train_data_1(data_root_path):\n",
    "    images = []\n",
    "    train_data_path = data_root_path + 'train'\n",
    "    y = []\n",
    "    for i in range(1,6):\n",
    "        #print(i)\n",
    "        file_path = train_data_path + '/data_batch_' + str(i)\n",
    "        dictionary = unpickle(file_path)\n",
    "        X_temp = dictionary[b'data']\n",
    "        X_temp = X_temp.T\n",
    "        X_temp = np.true_divide(X_temp,255)\n",
    "        #print(X_temp.shape)\n",
    "        y_temp = dictionary[b'labels']\n",
    "        #print(len(y_temp))\n",
    "        images.append(X_temp)\n",
    "        y = y + y_temp\n",
    "    X = np.column_stack(images)\n",
    "    y = np.asarray(y)\n",
    "#     print(\"=============\")\n",
    "#     print(X.shape)\n",
    "#     print(len(y))\n",
    "    \n",
    "    return X, y\n",
    "        \n",
    "def get_label_mapping_1(label_file):\n",
    "    \n",
    "    dictionary = unpickle(label_file)\n",
    "    label2id = {}\n",
    "    id2label = dictionary[b'label_names']\n",
    "    count = 0\n",
    "    \n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "'''\n",
    "NOTE: DATA LOADING:\n",
    "Directory structure: data/cifar-10-batches-py/\n",
    "\n",
    "TRAIN: data/cifar-10-batches-py/train\n",
    "This folder contains all the data_batch_1 to data_batch_5 files\n",
    "\n",
    "TEST: data/cifar-10-batches-py/test\n",
    "This folder contains the test_batch file\n",
    "\n",
    "REST: data/cifar-10-batches-py/rest\n",
    "This folder conatins the rest of the batches.meta and Readme file.\n",
    "'''\n",
    "\n",
    "data_root_path_1 = 'data/cifar-10-batches-py/'\n",
    "X_train, y_train = get_train_data_1(data_root_path_1)\n",
    "print(\"X_train shape: \", X_train)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "id2label_1, label2id_1 = get_label_mapping_1('data/cifar-10-batches-py/rest/batches.meta')\n",
    "print(label2id_1)\n",
    "\n",
    "###############################################################\n",
    "layer_dimensions = [X_train.shape[0], 400, 250, 10]  # including the input and output layers\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "NN.train(X_train, y_train, iters=800, alpha=0.001, batch_size=100, print_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We know the above Training and Validation Accuracies are confusing.(Please READ)\n",
    "\n",
    "### The reason this happened was, in our \"train\" function while we created X_train and y_train which had 45000 samples after removing samples for validation set. Inside the inner for loop we forgot to update the variable names from X,y to X_train, y_train. So our model got trained on entire 50000 training samples while we printed Accuracies based on validation set size numbers. \n",
    "\n",
    "### But the accuracy that we got for the test set was accurate: 51.18%\n",
    "### So we ended up having a traditional \"train test model\" and not \"train, validation and test model\".\n",
    "\n",
    "### We were not able to re-run the code as this itself took us 5hrs to run and we were on a time crunch.\n",
    "\n",
    "### We hope you will consider this as an honest mistake for not following the train validation and test set format.\n",
    "### And we will make sure to not repeat this next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = unpickle('data/cifar-10-batches-py/test/test_batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We got a Test Accuracy of 49.14%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 10000)\n",
      "(10000,)\n",
      "Accuracy= 49.14\n"
     ]
    }
   ],
   "source": [
    "X_test = dictionary[b'data']\n",
    "\n",
    "X_test = X_test.T\n",
    "X_test = np.true_divide(X_test, 255)\n",
    "#print(X_temp.shape)\n",
    "y_test = dictionary[b'labels']\n",
    "print(X_test.shape)\n",
    "\n",
    "y_predicted = NN.predict(X_test)\n",
    "save_predictions('ans1-ung200-avs431', y_predicted)\n",
    "\n",
    "# test if your numpy file has been saved correctly\n",
    "loaded_y = np.load('ans1-ung200-avs431.npy')\n",
    "print(loaded_y.shape)\n",
    "loaded_y[:10]\n",
    "\n",
    "#y_validate2 = NN2.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy=\",accuracy_score(y_test,y_predicted)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
